Auto-configured hyperparameters: mb=32, mbstd=4, fmaps=0.5, lrate=0.0025, gamma=0.1024, ema=10.0
Batch size: 32
Batch size per GPU: 32
Disabling discriminator augmentation.

Training options:
{
  "num_gpus": 1,
  "image_snapshot_ticks": 1,
  "network_snapshot_ticks": 1,
  "metrics": [
    "uchida_extraction",
    "fid50k_full"
  ],
  "random_seed": 0,
  "training_set_kwargs": {
    "class_name": "training.dataset.ImageFolderDataset",
    "path": "/home/mzoughebi/personal_study/CelebA_adapted_128.zip",
    "use_labels": false,
    "max_size": 160000,
    "xflip": false,
    "resolution": 128
  },
  "data_loader_kwargs": {
    "pin_memory": true,
    "num_workers": 3,
    "prefetch_factor": 2
  },
  "G_kwargs": {
    "class_name": "training.networks.Generator",
    "z_dim": 512,
    "w_dim": 512,
    "mapping_kwargs": {
      "num_layers": 2
    },
    "synthesis_kwargs": {
      "channel_base": 16384,
      "channel_max": 512,
      "num_fp16_res": 4,
      "conv_clamp": 256
    }
  },
  "D_kwargs": {
    "class_name": "training.networks.Discriminator",
    "block_kwargs": {},
    "mapping_kwargs": {},
    "epilogue_kwargs": {
      "mbstd_group_size": 4
    },
    "channel_base": 16384,
    "channel_max": 512,
    "num_fp16_res": 4,
    "conv_clamp": 256
  },
  "G_opt_kwargs": {
    "class_name": "torch.optim.Adam",
    "lr": 0.0025,
    "betas": [
      0,
      0.99
    ],
    "eps": 1e-08
  },
  "D_opt_kwargs": {
    "class_name": "torch.optim.Adam",
    "lr": 0.0025,
    "betas": [
      0,
      0.99
    ],
    "eps": 1e-08
  },
  "loss_kwargs": {
    "class_name": "training.loss.StyleGAN2Loss",
    "r1_gamma": 0.1024,
    "watermarking_dict": true
  },
  "water_config_path": "/home/mzoughebi/personal_study/StyleGAN2-ADA-4_Watermarking_VF/configs/watermarking_dict_conf_UCHI.json",
  "total_kimg": 1,
  "batch_size": 32,
  "batch_gpu": 32,
  "ema_kimg": 10.0,
  "ema_rampup": null,
  "resume_pkl": "/home/mzoughebi/personal_study/weights/trained_16000.pkl",
  "ada_kimg": 100,
  "run_dir": "./training_run_test_UCHIDA/00007-CelebA_adapted_128-watermarking1-noaug-resumecustom"
}

Watermarking Configuration Method Path:  /home/mzoughebi/personal_study/StyleGAN2-ADA-4_Watermarking_VF/configs/watermarking_dict_conf_UCHI.json
Output directory:   ./training_run_test_UCHIDA/00007-CelebA_adapted_128-watermarking1-noaug-resumecustom
Training data:      /home/mzoughebi/personal_study/CelebA_adapted_128.zip
Training duration:  1 kimg
Number of GPUs:     1
Number of images:   160000
Image resolution:   128
Conditional model:  False
Dataset x-flips:    False

Creating output directory...
Launching processes...
Loading training set...

Num images:  160000
Image shape: [3, 128, 128]
Label shape: [0]

Constructing networks...
/home/mzoughebi/personal_study/StyleGAN2-ADA-4_Watermarking_VF/configs/watermarking_dict_conf_UCHI.json
EMA_KIMG: 1
KIMG_PER_TICK: 1
>>>> UCHIDA INIT <<<<<
Weight name: synthesis.b32.conv0.weight, size: torch.Size([512, 512, 3, 3])
WATERMARK FOR INSERTION:   tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1])
min X:  tensor(-0.0756, device='cuda:0') max X:  tensor(0.0642, device='cuda:0')
Loading watermarking_dict from /home/mzoughebi/personal_study/weights/trained_16000.pkl...
No watermarking_dict found in the resume file. Using default values.
>>>> UCHIDA INIT <<<<<
Weight name: synthesis.b32.conv0.weight, size: torch.Size([512, 512, 3, 3])
WATERMARK FOR INSERTION:   tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0])
min X:  tensor(-0.0602, device='cuda:0') max X:  tensor(0.0614, device='cuda:0')
Resuming from "/home/mzoughebi/personal_study/weights/trained_16000.pkl"
Setting up PyTorch plugin "bias_act_plugin"... Done.
Setting up PyTorch plugin "upfirdn2d_plugin"... Done.

Generator             Parameters  Buffers  Output shape         Datatype
---                   ---         ---      ---                  ---     
mapping.fc0           262656      -        [32, 512]            float32 
mapping.fc1           262656      -        [32, 512]            float32 
mapping               -           512      [32, 12, 512]        float32 
synthesis.b4.conv1    2622465     32       [32, 512, 4, 4]      float32 
synthesis.b4.torgb    264195      -        [32, 3, 4, 4]        float32 
synthesis.b4:0        8192        16       [32, 512, 4, 4]      float32 
synthesis.b4:1        -           -        [32, 512, 4, 4]      float32 
synthesis.b8.conv0    2622465     80       [32, 512, 8, 8]      float32 
synthesis.b8.conv1    2622465     80       [32, 512, 8, 8]      float32 
synthesis.b8.torgb    264195      -        [32, 3, 8, 8]        float32 
synthesis.b8:0        -           16       [32, 512, 8, 8]      float32 
synthesis.b8:1        -           -        [32, 512, 8, 8]      float32 
synthesis.b16.conv0   2622465     272      [32, 512, 16, 16]    float16 
synthesis.b16.conv1   2622465     272      [32, 512, 16, 16]    float16 
synthesis.b16.torgb   264195      -        [32, 3, 16, 16]      float16 
synthesis.b16:0       -           16       [32, 512, 16, 16]    float16 
synthesis.b16:1       -           -        [32, 512, 16, 16]    float32 
synthesis.b32.conv0   2622465     1040     [32, 512, 32, 32]    float16 
synthesis.b32.conv1   2622465     1040     [32, 512, 32, 32]    float16 
synthesis.b32.torgb   264195      -        [32, 3, 32, 32]      float16 
synthesis.b32:0       -           16       [32, 512, 32, 32]    float16 
synthesis.b32:1       -           -        [32, 512, 32, 32]    float32 
synthesis.b64.conv0   1442561     4112     [32, 256, 64, 64]    float16 
synthesis.b64.conv1   721409      4112     [32, 256, 64, 64]    float16 
synthesis.b64.torgb   132099      -        [32, 3, 64, 64]      float16 
synthesis.b64:0       -           16       [32, 256, 64, 64]    float16 
synthesis.b64:1       -           -        [32, 256, 64, 64]    float32 
synthesis.b128.conv0  426369      16400    [32, 128, 128, 128]  float16 
synthesis.b128.conv1  213249      16400    [32, 128, 128, 128]  float16 
synthesis.b128.torgb  66051       -        [32, 3, 128, 128]    float16 
synthesis.b128:0      -           16       [32, 128, 128, 128]  float16 
synthesis.b128:1      -           -        [32, 128, 128, 128]  float32 
---                   ---         ---      ---                  ---     
Total                 22949277    44448    -                    -       


Discriminator  Parameters  Buffers  Output shape         Datatype
---            ---         ---      ---                  ---     
b128.fromrgb   512         16       [32, 128, 128, 128]  float16 
b128.skip      32768       16       [32, 256, 64, 64]    float16 
b128.conv0     147584      16       [32, 128, 128, 128]  float16 
b128.conv1     295168      16       [32, 256, 64, 64]    float16 
b128           -           16       [32, 256, 64, 64]    float16 
b64.skip       131072      16       [32, 512, 32, 32]    float16 
b64.conv0      590080      16       [32, 256, 64, 64]    float16 
b64.conv1      1180160     16       [32, 512, 32, 32]    float16 
b64            -           16       [32, 512, 32, 32]    float16 
b32.skip       262144      16       [32, 512, 16, 16]    float16 
b32.conv0      2359808     16       [32, 512, 32, 32]    float16 
b32.conv1      2359808     16       [32, 512, 16, 16]    float16 
b32            -           16       [32, 512, 16, 16]    float16 
b16.skip       262144      16       [32, 512, 8, 8]      float16 
b16.conv0      2359808     16       [32, 512, 16, 16]    float16 
b16.conv1      2359808     16       [32, 512, 8, 8]      float16 
b16            -           16       [32, 512, 8, 8]      float16 
b8.skip        262144      16       [32, 512, 4, 4]      float32 
b8.conv0       2359808     16       [32, 512, 8, 8]      float32 
b8.conv1       2359808     16       [32, 512, 4, 4]      float32 
b8             -           16       [32, 512, 4, 4]      float32 
b4.mbstd       -           -        [32, 513, 4, 4]      float32 
b4.conv        2364416     16       [32, 512, 4, 4]      float32 
b4.fc          4194816     -        [32, 512]            float32 
b4.out         513         -        [32, 1]              float32 
---            ---         ---      ---                  ---     
Total          23882369    352      -                    -       

Setting up augmentation...
Distributing across 1 GPUs...
Setting up training phases...
Exporting sample images...
Initial images saved for watermarking diff map...
Initializing logs...
Training for 1 kimg...

[BATCH 0] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.702112
[BATCH 0] [ROUND 0 PHASE Greg] 
[BATCH 0] [ROUND 0 PHASE Dmain] 
[BATCH 0] [ROUND 0 PHASE Dreg] 
tick 0     kimg 0.0      time 16s          sec/tick 1.8     sec/kimg 57.60   maintenance 14.2   cpumem 4.54   gpumem 9.09   augment 0.000
Evaluating metrics...
model device cuda:0
{"results": {"uchida_bit_acc": 0.5, "uchida_hamming_dist": 50.0}, "metric": "uchida_extraction", "total_time": 0.022526264190673828, "total_time_str": "0s", "num_gpus": 1, "snapshot_pkl": "network-snapshot-000000.pkl", "timestamp": 1765383964.9635725}
[metrics] Loading feature detector from local file: /home/mzoughebi/personal_study/weights/inception-2015-12-05.pt
{"results": {"fid50k_full": 3.3506423236466865}, "metric": "fid50k_full", "total_time": 292.2347402572632, "total_time_str": "4m 52s", "num_gpus": 1, "snapshot_pkl": "network-snapshot-000000.pkl", "timestamp": 1765384257.1986945}
[BATCH 1] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.696615
[BATCH 1] [ROUND 0 PHASE Dmain] 
[BATCH 2] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.690651
[BATCH 2] [ROUND 0 PHASE Dmain] 
[BATCH 3] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.685167
[BATCH 3] [ROUND 0 PHASE Dmain] 
[BATCH 4] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.679444
[BATCH 4] [ROUND 0 PHASE Greg] 
[BATCH 4] [ROUND 0 PHASE Dmain] 
[BATCH 5] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.674378
[BATCH 5] [ROUND 0 PHASE Dmain] 
[BATCH 6] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.669187
[BATCH 6] [ROUND 0 PHASE Dmain] 
[BATCH 7] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.664080
[BATCH 7] [ROUND 0 PHASE Dmain] 
[BATCH 8] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.658956
[BATCH 8] [ROUND 0 PHASE Greg] 
[BATCH 8] [ROUND 0 PHASE Dmain] 
[BATCH 9] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.654145
[BATCH 9] [ROUND 0 PHASE Dmain] 
[BATCH 10] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.649075
[BATCH 10] [ROUND 0 PHASE Dmain] 
[BATCH 11] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.644286
[BATCH 11] [ROUND 0 PHASE Dmain] 
[BATCH 12] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.639378
[BATCH 12] [ROUND 0 PHASE Greg] 
[BATCH 12] [ROUND 0 PHASE Dmain] 
[BATCH 13] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.634711
[BATCH 13] [ROUND 0 PHASE Dmain] 
[BATCH 14] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.629974
[BATCH 14] [ROUND 0 PHASE Dmain] 
[BATCH 15] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.625404
[BATCH 15] [ROUND 0 PHASE Dmain] 
[BATCH 16] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.620799
[BATCH 16] [ROUND 0 PHASE Greg] 
[BATCH 16] [ROUND 0 PHASE Dmain] 
[BATCH 16] [ROUND 0 PHASE Dreg] 
[BATCH 17] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.616370
[BATCH 17] [ROUND 0 PHASE Dmain] 
[BATCH 18] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.611903
[BATCH 18] [ROUND 0 PHASE Dmain] 
[BATCH 19] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.607571
[BATCH 19] [ROUND 0 PHASE Dmain] 
[BATCH 20] [ROUND 0 PHASE Gmain] 
[WB LOSS] Mean=0.603144

Aborted!
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4e23aad430>
Traceback (most recent call last):
  File "/home/mzoughebi/miniconda3/envs/SG2-ada-2080ti/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1324, in __del__
    self._shutdown_workers()
  File "/home/mzoughebi/miniconda3/envs/SG2-ada-2080ti/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1297, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/mzoughebi/miniconda3/envs/SG2-ada-2080ti/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/mzoughebi/miniconda3/envs/SG2-ada-2080ti/lib/python3.8/multiprocessing/popen_fork.py", line 44, in wait
    if not wait([self.sentinel], timeout):
  File "/home/mzoughebi/miniconda3/envs/SG2-ada-2080ti/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/mzoughebi/miniconda3/envs/SG2-ada-2080ti/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt: 
